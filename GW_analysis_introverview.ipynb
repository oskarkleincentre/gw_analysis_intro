{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Overview to Using LIGO/Virgo GW-strain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These imports are more to check that you have the correct packages installed in your environment for use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gwpy\n",
    "import gwosc\n",
    "import pycbc\n",
    "import lalinference\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for event information\n",
    "\n",
    "The gwosc.datasets module provides tools to search for datasets, including filtering on GPS times.\n",
    "\n",
    "For example, we can search for what event datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc.datasets import find_datasets\n",
    "events = find_datasets(type=\"event\", detector=\"H1\", segment=(1126051217, 1137254417))\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find the gps times for different science operations we can use the `run_segment` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc.datasets import run_segment\n",
    "print(run_segment('O1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore to access data from different events that are recorded we can find the urls where to download the data such for the first event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc.locate import get_event_urls\n",
    "urls = get_event_urls('GW150914') # detector = , duration = )\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this data we can download and read it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = str(urls[0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.gw-osc.org/catalog/GWTC-1-confident/data/GW150914/H-H1_GWOSC_4KHZ_R1-1126259447-32.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwpy.timeseries import TimeSeries\n",
    "hdata_down = TimeSeries.read('H-H1_GWOSC_4KHZ_R1-1126259447-32.hdf5', format='hdf5.losc')\n",
    "type(hdata_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data querying exercises\n",
    "\n",
    "- How long did O2 last, in human-readable format, i.e. (days, months, years)?\n",
    "- How many events were detected during O1?\n",
    "- What file URL contains data for V1 4096 seconds around GW170817?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strain data visualization\n",
    "\n",
    "Now let's see what GW strain data looks like. We will again use the gwosc package to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc.datasets import event_gps\n",
    "gps = event_gps('GW170817')\n",
    "print(gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = (int(gps)-5, int(gps)+5)\n",
    "print(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdata = TimeSeries.fetch_open_data('H1', *segment, verbose=True)\n",
    "print(hdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TimeSeries class comes with a load of useful methods for doing basic operations with the GW-strain data, particularly plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = hdata.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and plot data\n",
    "\n",
    "- Using the methods discussed in parts one and two, download the data from the Livingston detector for `GW150914` and plot the strain for a segement of 30 seconds around the merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency domain properties of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, let's use the data from `GW150914` to begin interrogating the properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data form the Livingston detect or ldata\n",
    "ldata = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a basic fft of the data and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = ldata.fft()\n",
    "print(fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the above object is no longer an instance of our TimeSeries object that we originally read the data in as. It is now a FrequencySeries object, related, but with different properties and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = fft.abs().plot(xscale=\"log\", yscale=\"log\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is the amplitude spectral density, it doesn't look terribly much like the typical LIGO/Virgo like sensitivity plots that we see. This is because we need to window and bandpass the data. However, instead of doing all these steps separately, the TimeSeries and Frequency series objects have methods built-in to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = ldata.asd(fftlength=4, method=\"median\")\n",
    "plot = asd.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(10, 1400)\n",
    "ax.set_ylim(2e-24, 1e-20)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with such short data lengths, there is a lot of noise in the ASD. Given a longer set of data, can you plot the ASD around a different event?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD Exercise\n",
    "\n",
    "- Obtain data from a different event, let's say `GW170817` and plot the ASD for both the Hanford and Livingston detectors. Get a section of data longer than 120s.\n",
    "\n",
    "The code below will take care of the plotting for you, please fill in teh steps to retrieve the data and perform the ASD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = \n",
    "# get Hanford data\n",
    "hdata2 = \n",
    "hasd2 = \n",
    "# get Livingston data\n",
    "ldata2 = \n",
    "lasd2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = hasd2.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(10, 1400)\n",
    "ax.set_ylim(5e-24, 1e-20)\n",
    "# and plot using standard colours\n",
    "ax.plot(lasd2, label='LIGO-Livingston', color='gwpy:ligo-livingston')\n",
    "# update the Hanford line to use standard colour, and have a label\n",
    "hline = ax.lines[0]\n",
    "hline.set_color('gwpy:ligo-hanford')  # change colour of Hanford data\n",
    "hline.set_label('LIGO-Hanford')\n",
    "\n",
    "ax.set_ylabel(r'Strain noise [$1/\\sqrt{\\mathrm{Hz}}$]')\n",
    "ax.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-tranforms and ASD spectrograms\n",
    "\n",
    "Another, very interesting way to interrogate GW data is too look at the famous Q-transform plots which effectively show PSD evolution with time. Using the data we already have let's look at one of these 'spectrograms' which is a more basic form of the Q-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specgram = ldata.spectrogram2(fftlength=4, overlap=2, window='hann') ** (1/2.)\n",
    "plot = specgram.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(10, 1400)\n",
    "ax.colorbar(\n",
    "    clim=(1e-24, 1e-20),\n",
    "    norm=\"log\",\n",
    "    label=r\"Strain noise [$1/\\sqrt{\\mathrm{Hz}}$]\",\n",
    ")\n",
    "plot.show()  # refresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fine, but it doesn't show the nice 'tracks' that became some famous with the first detection `GW150914` and `GW170817`. This requires a Q-transform plot. With getting into the details, a Q-transform plot essentially finds the optimal window size for each time which returns the highest Q value of that PSD. This allows us to pick out the highest power features as they sweep through frequency space while evolving with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = event_gps('GW170817')\n",
    "segment = (int(gps) - 30, int(gps) + 2)\n",
    "hdata = TimeSeries.fetch_open_data('H1', *segment, verbose=True, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully this q-transform is another hand method of the GWpy TimeSeries object, so all we need to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = hdata.q_transform(frange=(30, 500))\n",
    "plot = hq.plot()\n",
    "plot.colorbar(label=\"Normalised energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can almost see a track in the data. However, we can optimze our choice of the range of allowed q to visualize this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = hdata.q_transform(frange=(30, 500), qrange=(100, 110))\n",
    "plot = hq.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_epoch(gps)\n",
    "ax.set_yscale('log')\n",
    "ax.colorbar(label=\"Normalised energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-transform exercise\n",
    "\n",
    "- Can you repeat the above for `GW170817` but for the Livngston detector? What do you see?\n",
    "- Can you modify the tranform or the visualization to better see the inspiral in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating GW signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we find a signal in the data? This is done via a process called matched filtering. To do matched filtering we will use the `PyCBC` package. Before we do any fancy searching for signals let's see some of the cool things we can do with `PyCBC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.waveform import get_td_waveform, td_approximants\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyCBC` has a myraid of different waveform templates that we can generate mock signals from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The options for waveforms in the timedomain are the following: \n",
    "print(td_approximants())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These represent many different approximants from theory and numerical simulations. With different effects captured by them. Whether PN approximation orders or tidal effects or numerically tuned waveforms, there is a lot of variety. The one we will use below is a very typical one when considering mergers of binary black holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of this function are the \"plus\" and \"cross\" polarizations of the gravitational-wave signal \n",
    "# as viewed from the line of sight at a given source inclination (assumed face-on if not provided)\n",
    "hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\", # The waveform approximant\n",
    "                         mass1=10, # Solar masses of the binary componenets\n",
    "                         mass2=10, \n",
    "                         distance= 100, #Mpc\n",
    "                         delta_t=1.0/4096, #4096 Hz sampling\n",
    "                         f_lower=30) # Lower frequency cutoff to begin generating the waveform.\n",
    "                                     # Effectively this sets the starting point of the timeseries.\n",
    "\n",
    "pylab.plot(hp.sample_times, hp, label='Plus Polarization')\n",
    "pylab.plot(hp.sample_times, hc, label='Cross Polarization')\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.legend()\n",
    "pylab.grid()\n",
    "pylab.show()\n",
    "\n",
    "# Zoom in near the merger time#\n",
    "pylab.plot(hp.sample_times, hp, label='Plus Polarization')\n",
    "pylab.plot(hp.sample_times, hc, label='Cross Polarization')\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.xlim(-.01, .01)\n",
    "pylab.legend()\n",
    "pylab.grid()\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our insprial now, with both polarizations, however, as we are not terribly sensitive to polarizations of GW signals yet with the current generation of detectors it is perfectly fine to select one polarization and use that as the complete wave.\n",
    "\n",
    "An interesting thing to interrogate is how do these waveforms change with masses, distances, and inclinations of the binary system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_amp = []\n",
    "distances = np.arange(100,1000,10)\n",
    "for d in distances:\n",
    "    hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
    "                         mass1=10,\n",
    "                         mass2=10,\n",
    "                         delta_t=1.0/4096,\n",
    "                         inclination = 0.0,\n",
    "                         f_lower=30,\n",
    "                         distance=d)\n",
    "    max_amp.append(max(abs(hp)))\n",
    "\n",
    "pylab.plot(distances,max_amp, label=\"GW-Strain\")\n",
    "pylab.plot(distances, 2.5e-19/distances, label=\"1/d fall-off\")\n",
    "pylab.xlabel('Distance [Mpc]')\n",
    "pylab.legend()\n",
    "pylab.ylabel('GW-strain maximum amp')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveform properties exercise\n",
    "\n",
    "- Can you make a similar plot to the above for the variation of amplitude with inclination?\n",
    "- \"\" with changing the mass of one of the binaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock data with noise\n",
    "\n",
    "Now let's generate some mock data with noise and perform mathced filtering on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 4096 # samples per second\n",
    "data_length = 1024 # seconds\n",
    "\n",
    "# Generate a long stretch of white noise\n",
    "white_noise = 1.0e-19*np.random.normal(size=[sample_rate * data_length])\n",
    "times = np.arange(len(white_noise)) / float(sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp1, _ = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
    "                         mass1=10,\n",
    "                         mass2=10,\n",
    "                         delta_t=1.0/sample_rate,\n",
    "                         f_lower=25)\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title(\"The waveform hp1\")\n",
    "pylab.plot(hp1.sample_times, hp1)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Amplitude')\n",
    "\n",
    "waveform_start = numpy.random.randint(0, len(white_noise) - len(hp1))\n",
    "white_noise[waveform_start:waveform_start+len(hp1)] += hp1.numpy()\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title(\"Looks like random noise, right?\")\n",
    "pylab.plot(hp1.sample_times, white_noise[waveform_start:waveform_start+len(hp1)])\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Amplitude')\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title(\"Signal in the data\")\n",
    "pylab.plot(hp1.sample_times, white_noise[waveform_start:waveform_start+len(hp1)])\n",
    "pylab.plot(hp1.sample_times, hp1)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Normalized amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat the process, but with colored noise from the expected noise spectrum on the `Hanford` detector in O3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycbc.noise\n",
    "import pycbc.psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp1, _ = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
    "                         mass1=10,\n",
    "                         mass2=10,\n",
    "                         delta_t=1.0/sample_rate,\n",
    "                         distance=100,\n",
    "                         f_lower=25)\n",
    "\n",
    "# The color of the noise matches a PSD which you provide\n",
    "flow = 25.0\n",
    "delta_f = 1.0 / 16\n",
    "flen = int(sample_rate / delta_f) + 1\n",
    "psd = pycbc.psd.aLIGOaLIGOO3LowT1800545(flen, delta_f, flow)\n",
    "\n",
    "# Generate 32 seconds of noise at 4096 Hz\n",
    "delta_t = 1.0 / 4096\n",
    "tsamples = int(32 / delta_t)\n",
    "ts = pycbc.noise.noise_from_psd(tsamples, delta_t, psd, seed=127)\n",
    "colored_noise = ts\n",
    "\n",
    "pylab.plot(ts.sample_times, colored_noise)\n",
    "pylab.ylabel('Strain')\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()\n",
    "\n",
    "waveform_start = numpy.random.randint(0, len(colored_noise) - len(hp1))\n",
    "colored_noise[waveform_start:waveform_start+len(hp1)] += hp1.numpy()\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title(\"Looks like random noise, right?\")\n",
    "pylab.plot(hp1.sample_times, colored_noise[waveform_start:waveform_start+len(hp1)])\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Normalized amplitude')\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title(\"Signal in the data\")\n",
    "pylab.plot(hp1.sample_times, colored_noise[waveform_start:waveform_start+len(hp1)])\n",
    "pylab.plot(hp1.sample_times, hp1)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Normalized amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact if you see above we have added in the signal to the noise. Now let's perform matched filtering back on this signal to see if we can extract the waveform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock data generation exercises\n",
    "\n",
    "Using the above:\n",
    "- Can you generate a timeseries of colored noise (pick whatever PSD you want, tab complete psd.... and it will show many options) with a BNS inspiral in the data? For a BNS using 1.4 solar masses for each component and the waveform approximant `TaylorF2`. The other options are up to you, but I would recommend a starting frequency no higher than 30 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BNS waveform generated in the above exercise and noisy data we can perform matched filtering in essentially one easy step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set BNS waveform as template, using plus polarization\n",
    "template =\n",
    "# Set the noisy data as the strain\n",
    "det_strain = \n",
    "# Given the psd you used to generate the above noise data\n",
    "det_psd =\n",
    "# If your starting frequency was above 20 Hz set that as the cut freqnecy otherwise use 20Hz\n",
    "f_cut ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.filter import matched_filter\n",
    "# Now use this template to perform matched filtering against the simulated signal\n",
    "snr = matched_filter(\n",
    "    template, det_strain, psd=det_psd, low_frequency_cutoff=f_cut\n",
    ")\n",
    "snr.crop(15, 6)\n",
    "peak = abs(snr).numpy().argmax()\n",
    "snrp = snr[peak]\n",
    "time = snr.sample_times[peak]\n",
    "# The SNR is techincally complex but we use the abs_max as the report SNR of the signal\n",
    "print(\"This signal is detected with an SNR of \", snrp)\n",
    "print(\"This occurred at a time of \", time)\n",
    "\n",
    "pylab.plot(det_strain.sample_times,snr)\n",
    "pylab.xlabel('Times (s)')\n",
    "pylab.ylabel('Matched Filtering SNR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched filtering on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course mathced filtering is normally nowhere this simple. We knew the signal, we had well-described colored gaussian noise, and by generation itself we did not need to bandpass or window the data. Let's finish by trying to perform matched filtering on a real dataset. The following example borrows heavily from the online tutorial of PyCBC matched filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.catalog import Merger\n",
    "from pycbc.filter import resample_to_delta_t, highpass\n",
    "\n",
    "# As an example we use the GW150914 data\n",
    "merger = Merger(\"GW150914\")\n",
    "\n",
    "# Get the data from the Hanford detector\n",
    "strain = merger.strain('H1')\n",
    "\n",
    "# Remove the low frequency content and downsample the data to 2048Hz\n",
    "strain = highpass(strain, 15.0)\n",
    "strain = resample_to_delta_t(strain, 1.0/2048)\n",
    "\n",
    "pylab.plot(strain.sample_times, strain)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some weird edge effects happening. This is due to the finite size of our data set and the filter wraparound that occures when highpassing the data. Thus we must crop the ends of the data to remove this artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 2 seconds of data from both the beginning and end\n",
    "conditioned = strain.crop(2, 2)\n",
    "\n",
    "pylab.plot(conditioned.sample_times, conditioned)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine place to start to estimate your noise is to use the predicted PSD's as we did above. However, in practice the noise does change from day to day, season to season, and even on approximately 15 minute timescale it can change somewhat. Thus we should estimate the PSD from the data set that have. As we are only concerned about the region over which the LIGO/Virgo data is calibrated [20 Hz, ~2000 Hz] we can estimate the PSD using Welch's method and taking many windows of the data over intervals much greater than our lower limiting frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.psd import interpolate, inverse_spectrum_truncation\n",
    "# Estimate the power spectral density\n",
    "\n",
    "# We use 4 second samples of our time series in Welch method.\n",
    "psd = conditioned.psd(4)\n",
    "\n",
    "# Now that we have the psd we need to interpolate it to match our data\n",
    "# and then limit the filter length of 1 / PSD. After this, we can\n",
    "# directly use this PSD to filter the data in a controlled manner\n",
    "psd = interpolate(psd, conditioned.delta_f)\n",
    "\n",
    "# 1/PSD will now act as a filter with an effective length of 4 seconds\n",
    "# Since the data has been highpassed above 15 Hz, and will have low values\n",
    "# below this we need to inform the function to not include frequencies\n",
    "# below this frequency. \n",
    "psd = inverse_spectrum_truncation(psd, 4 * conditioned.sample_rate,\n",
    "                                  low_frequency_cutoff=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate a template to use against the data, in practice a whole template bank will be used to search for many potential signals, but as we know that we are looking for BBH given this is `GW150914` we can use an approximant that is quite similar to the true signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume equal masses, and non-rotating black holes which is within the posterior probability\n",
    "# of GW150914. \n",
    "m = 36 # Solar masses\n",
    "hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
    "                     mass1=m,\n",
    "                     mass2=m,\n",
    "                     delta_t=conditioned.delta_t,\n",
    "                     f_lower=20)\n",
    "\n",
    "# We will resize the vector to match our data\n",
    "hp.resize(len(conditioned))\n",
    "\n",
    "# The waveform begins at the start of the vector, so if we want the\n",
    "# SNR time series to correspond to the approximate merger location\n",
    "# we need to shift the data so that the merger is approximately at the \n",
    "# first bin of the data.\n",
    "\n",
    "# The cyclic_time_shift method shifts the timeseries by a given amount of time.\n",
    "# It treats the data as if it were on a ring so points shifted off the end\n",
    "# of the series reappear at the start. Note that time stamps are *not* in\n",
    "# general affected (as the start time of the full array is shifted),\n",
    "# but the index of each point in the vector is.\n",
    "#\n",
    "# By convention waveforms returned from `get_td_waveform` have their\n",
    "# merger stamped with time zero, so we can use the start time to \n",
    "# shift the merger into position\n",
    "pylab.figure()\n",
    "pylab.title('Before shifting')\n",
    "pylab.plot(hp.sample_times, hp)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Strain')\n",
    "\n",
    "template = hp.cyclic_time_shift(hp.start_time)\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title('After shifting')\n",
    "pylab.plot(template.sample_times, template)\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.ylabel('Strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = matched_filter(template, conditioned,\n",
    "                     psd=psd, low_frequency_cutoff=20)\n",
    "\n",
    "# Remove time corrupted by the template filter and the psd filter\n",
    "# We remove 4 seonds at the beginning and end for the PSD filtering\n",
    "# And we remove 4 additional seconds at the beginning to account for\n",
    "# the template length (this is somewhat generous for \n",
    "# so short a template). A longer signal such as from a BNS, would \n",
    "# require much more padding at the beginning of the vector.\n",
    "snr = snr.crop(4 + 4, 4)\n",
    "\n",
    "# Why are we taking an abs() here?\n",
    "# The `matched_filter` function actually returns a 'complex' SNR.\n",
    "# What that means is that the real portion correponds to the SNR\n",
    "# associated with directly filtering the template with the data.\n",
    "# The imaginary portion corresponds to filtering with a template that\n",
    "# is 90 degrees out of phase. Since the phase of a signal may be \n",
    "# anything, we choose to maximize over the phase of the signal.\n",
    "pylab.figure(figsize=[10, 4])\n",
    "pylab.plot(snr.sample_times, abs(snr))\n",
    "pylab.ylabel('Signal-to-noise')\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()\n",
    "\n",
    "peak = abs(snr).numpy().argmax()\n",
    "snrp = snr[peak]\n",
    "time = snr.sample_times[peak]\n",
    "\n",
    "print(\"We found a signal at {}s with SNR {}\".format(time, \n",
    "                                                    abs(snrp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched filtering exercise\n",
    "\n",
    "- Can you repeat the above process to find a merger signal for a different event? \n",
    "\n",
    "- As a challenge instead of getting the data directly from the Merger or Event methods, can you use everything we have learned thus far, download the data for an arbitrary segment size of data around your chosen event, read it in, plot the data, then proceed with the matched filtering? If you downloading an `hdf5` and reading in as described above it will be useful to use the `GWpy` method `to_pycbc` method to convert the data object into the class instance needed for the matched filtering example above. Best of luck!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
